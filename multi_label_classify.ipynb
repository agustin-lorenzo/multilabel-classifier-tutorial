{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-label classification with DistilBERT\n",
    "CSCI 6380 Tutorial\n",
    "\n",
    "Agustin Lorenzo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background\n",
    "This notebook outlines the process behind training a multilabel classifier. This uses HuggingFace's openly available DistilBERT transformer model, which is a lightweight version of BERT. With a multilabel classifier, instances can be classified under more than one label. This is ideal if you want to describe instances with concepts that aren't mutually exclusive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ael91697/mambaforge/envs/multilabel/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from transformers import Trainer\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score, f1_score, hamming_loss\n",
    "from transformers import EvalPrediction\n",
    "from transformers import TrainingArguments, Trainer\n",
    "#import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data\n",
    "Reading the .csv file and converting to proper datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_df = pd.read_csv('sample_train_split.csv') # splits are saved outside of the program so specific instances can be analyzed later\n",
    "test_df = pd.read_csv('sample_test_split.csv')\n",
    "\n",
    "# Convert labels from string back to list\n",
    "train_df['labels'] = train_df['labels'].str.strip('\"').str.split(', ')\n",
    "test_df['labels'] = test_df['labels'].str.strip('\"').str.split(', ')\n",
    "\n",
    "# Extract texts and labels\n",
    "train_texts = train_df['entry'].tolist()\n",
    "test_texts = test_df['entry'].tolist()\n",
    "train_labels = train_df['labels'].tolist()\n",
    "test_labels = test_df['labels'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding data\n",
    "Getting it in the correct format for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Label encoder\n",
    "multilabel = MultiLabelBinarizer()\n",
    "train_labels = multilabel.fit_transform(train_labels).astype('float32')\n",
    "test_labels = multilabel.transform(test_labels).astype('float32')\n",
    "\n",
    "checkpoint = \"distilbert/distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(checkpoint)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(checkpoint, num_labels=len(train_labels[0]), problem_type=\"multi_label_classification\")\n",
    "\n",
    "# encode data\n",
    "class CaseNoteDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=250):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = torch.tensor(self.labels[idx])\n",
    "        \n",
    "        encoding = self.tokenizer(text, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors='pt')\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': label\n",
    "        }\n",
    "\n",
    "# create final dataset objects with encoded data\n",
    "train_dataset = CaseNoteDataset(train_texts, train_labels, tokenizer)\n",
    "test_dataset = CaseNoteDataset(test_texts, test_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_label_metrics(predictions, labels, threshold=0.3):\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    \n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    y_true = labels\n",
    "    \n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average='macro') \n",
    "    hamming = hamming_loss(y_true, y_pred)\n",
    "\n",
    "    metrics = {\n",
    "        \"roc_auc\": roc_auc,\n",
    "        \"hamming_loss\": hamming,\n",
    "        \"f1\": f1\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def compute_metrics(p:EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    \n",
    "    result = multi_label_metrics(predictions=preds, labels=p.label_ids)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining parameters for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ael91697/mambaforge/envs/multilabel/lib/python3.13/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "metric_name = \"f1\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"bert-finetuned-sem_eval-english\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=1e-5,  # Set an initial learning rate\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    #report_to=\"wandb\",\n",
    "    report_to=[],\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    eval_steps=1\n",
    ")\n",
    "\n",
    "#wandb.init(project=\"case-notes-classification\", name=\"distilbert-base\")\n",
    "# wandb is a good resource for recording/presenting stats from model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:14, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Hamming Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.699700</td>\n",
       "      <td>0.693376</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>197.959000</td>\n",
       "      <td>49.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.697300</td>\n",
       "      <td>0.685188</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>198.073000</td>\n",
       "      <td>49.518000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.691400</td>\n",
       "      <td>0.676985</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.024200</td>\n",
       "      <td>165.252000</td>\n",
       "      <td>41.313000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.685900</td>\n",
       "      <td>0.667953</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>204.401000</td>\n",
       "      <td>51.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.672600</td>\n",
       "      <td>0.660572</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.025100</td>\n",
       "      <td>159.323000</td>\n",
       "      <td>39.831000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.668800</td>\n",
       "      <td>0.654467</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>197.915000</td>\n",
       "      <td>49.479000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.654500</td>\n",
       "      <td>0.648852</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.025200</td>\n",
       "      <td>159.021000</td>\n",
       "      <td>39.755000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.643400</td>\n",
       "      <td>0.644534</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>187.029000</td>\n",
       "      <td>46.757000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.670200</td>\n",
       "      <td>0.641630</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>194.246000</td>\n",
       "      <td>48.561000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.632600</td>\n",
       "      <td>0.640435</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.016600</td>\n",
       "      <td>240.292000</td>\n",
       "      <td>60.073000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#\n",
    "# wandb.init(project=\"case-notes-classification\", name=\"distilbert-base\")\n",
    "\n",
    "trainer = Trainer(model=model, \n",
    "                  args=args,\n",
    "                  train_dataset=train_dataset, \n",
    "                  eval_dataset=test_dataset,\n",
    "                  compute_metrics=compute_metrics,)\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "trainer.save_model(\"distilbert-finetuned\")\n",
    "with open(\"binarizer.bin\", \"wb\") as f:\n",
    "    pickle.dump(multilabel, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the model\n",
    "Now that the model has been trained, we can run it and give it a new instance to classify. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: (' plural', ' singular')\n"
     ]
    }
   ],
   "source": [
    "with open(\"binarizer.bin\", \"rb\") as f:\n",
    "    multilabel_binarizer = pickle.load(f)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "new_instance = \"black chair\"\n",
    "inputs = tokenizer(new_instance, return_tensors=\"pt\") # tokenize string for proper input\n",
    "\n",
    "# model outputs\n",
    "outputs = model(**inputs)\n",
    "predicted_probabilities = outputs.logits.sigmoid().detach().numpy()\n",
    "binary_predictions = (predicted_probabilities > 0.1).astype(int) # threshold = 0.5\n",
    "predicted_labels = multilabel.inverse_transform(binary_predictions)[0]\n",
    "\n",
    "print(\"Predicted labels:\", predicted_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16 instances is far from enough for the model to learn relationships between labels and entries, but with enough data this would be effective. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multilabel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
